{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e161ca-9551-493d-ac39-7b81fe930b08",
   "metadata": {},
   "source": [
    "**Q1. Explain the concept of homogeneity and completeness in clustering evaluation. How are they\n",
    "calculated?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b45d73-0c36-480a-b31d-8b0e82013e58",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Homogeneity and completeness are two important metrics used to evaluate the quality of clustering results, particularly in the context of evaluating how well clusters correspond to known ground truth labels or classes.\n",
    "\n",
    "### 1. Homogeneity:\n",
    "\n",
    "- **Definition:** Homogeneity measures if all clusters contain only data points that are members of a single class.\n",
    "- **Goal:** A high homogeneity score indicates that each cluster predominantly contains data points from a single class.\n",
    "\n",
    "### Calculation:\n",
    "\n",
    "Homogeneity \\( H \\) is calculated using the following formula:\n",
    "\n",
    "\\[ H = 1 - \\frac{H(C|K)}{H(K)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( H(C|K) \\) is the conditional entropy of the cluster assignment given the true class labels.\n",
    "- \\( H(K) \\) is the entropy of the true class labels.\n",
    "\n",
    "The conditional entropy \\( H(C|K) \\) is computed as:\n",
    "\n",
    "\\[ H(C|K) = - \\sum_{c=1}^{C} \\sum_{k=1}^{K} \\frac{n_{ck}}{n} \\log \\frac{n_{ck}}{n_k} \\]\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the number of clusters,\n",
    "- \\( K \\) is the number of classes,\n",
    "- \\( n \\) is the total number of data points,\n",
    "- \\( n_{ck} \\) is the number of data points that are in cluster \\( c \\) and belong to class \\( k \\),\n",
    "- \\( n_k \\) is the number of data points that belong to class \\( k \\).\n",
    "\n",
    "### 2. Completeness:\n",
    "\n",
    "- **Definition:** Completeness measures if all data points that are members of a given class are assigned to the same cluster.\n",
    "- **Goal:** A high completeness score indicates that all data points from the same class are grouped into the same cluster.\n",
    "\n",
    "### Calculation:\n",
    "\n",
    "Completeness \\( C \\) is calculated using the following formula:\n",
    "\n",
    "\\[ C = 1 - \\frac{H(K|C)}{H(K)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( H(K|C) \\) is the conditional entropy of the class labels given the cluster assignment.\n",
    "\n",
    "The conditional entropy \\( H(K|C) \\) is computed as:\n",
    "\n",
    "\\[ H(K|C) = - \\sum_{k=1}^{K} \\sum_{c=1}^{C} \\frac{n_{ck}}{n} \\log \\frac{n_{ck}}{n_c} \\]\n",
    "\n",
    "Where:\n",
    "- \\( n_{ck} \\) is the number of data points that are in cluster \\( c \\) and belong to class \\( k \\),\n",
    "- \\( n_c \\) is the number of data points that are in cluster \\( c \\).\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Homogeneity:** Measures the purity of clusters. A high homogeneity score indicates that clusters contain data points that are mostly from the same class.\n",
    "- **Completeness:** Measures how well all members of a class are assigned to the same cluster. A high completeness score indicates that all data points from the same class are grouped into the same cluster.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "- **Combined Metric:** The V-measure, which is the harmonic mean of homogeneity and completeness (\\( V = 2 \\cdot \\frac{H \\cdot C}{H + C} \\)), provides a balanced measure that considers both aspects of clustering quality.\n",
    "  \n",
    "- **Evaluation:** These metrics are particularly useful when evaluating clustering algorithms against ground truth or labeled data, helping to understand how well clusters correspond to known classes or groups in the data.\n",
    "\n",
    "In summary, homogeneity and completeness are essential metrics for evaluating clustering quality, providing insights into how well clusters reflect the underlying structure or classes in the data. They are calculated based on information theory principles, focusing on the purity and completeness of cluster assignments relative to known class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ec456-21ea-4e5a-a95f-5c998a25495a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d0213cb-b159-45db-bce6-fcde12dbbb38",
   "metadata": {},
   "source": [
    "**Q2. What is the V-measure in clustering evaluation? How is it related to homogeneity and completeness?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9003e13-21fd-4d01-90dc-77d9110ccced",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The V-measure is a clustering evaluation metric that combines homogeneity and completeness into a single measure to assess the quality of clustering results. It provides a balanced assessment by considering both how pure the clusters are (homogeneity) and how well all members of a given class are clustered together (completeness).\n",
    "\n",
    "### Definition:\n",
    "\n",
    "The V-measure \\( V \\) is defined as the harmonic mean of homogeneity \\( H \\) and completeness \\( C \\):\n",
    "\n",
    "\\[ V = \\frac{(1 + \\beta) \\cdot H \\cdot C}{\\beta \\cdot H + C} \\]\n",
    "\n",
    "Where:\n",
    "- \\( H \\) is the homogeneity,\n",
    "- \\( C \\) is the completeness,\n",
    "- \\( \\beta \\) is a parameter that weights the importance of homogeneity versus completeness. Typically, \\( \\beta = 1 \\) is used for equal weighting.\n",
    "\n",
    "### Relationship to Homogeneity and Completeness:\n",
    "\n",
    "- **Homogeneity \\( H \\):** Measures if all clusters contain only data points that are members of a single class.\n",
    "  \n",
    "- **Completeness \\( C \\):** Measures if all data points that are members of a given class are assigned to the same cluster.\n",
    "\n",
    "- **V-measure \\( V \\):** Harmonic mean of homogeneity and completeness. It balances these two aspects of clustering quality:\n",
    "  - When \\( \\beta = 1 \\), \\( V \\) becomes the harmonic mean of \\( H \\) and \\( C \\):\n",
    "    \\[ V = 2 \\cdot \\frac{H \\cdot C}{H + C} \\]\n",
    "  - \\( V \\) ranges from 0 to 1, where 0 indicates no clustering agreement with the ground truth labels, and 1 indicates perfect clustering agreement.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Balanced Measure:** \\( V \\)-measure provides a balanced assessment of clustering quality by considering both the purity of clusters (homogeneity) and the completeness of clustering (how well all members of a class are grouped together).\n",
    "  \n",
    "- **Evaluation:** Higher \\( V \\)-measure values indicate better clustering results where clusters closely match the known class labels or ground truth.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "- **Evaluation of Clustering Algorithms:** Use \\( V \\)-measure to compare different clustering algorithms or parameter settings based on how well they capture the underlying structure or classes in the data.\n",
    "  \n",
    "- **Interpretability:** Helps in understanding the trade-off between cluster purity and the completeness of class assignment when evaluating clustering results.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Parameter \\( \\beta \\):** Adjusting \\( \\beta \\) allows for emphasizing either homogeneity or completeness more strongly based on the specific application requirements or characteristics of the dataset.\n",
    "  \n",
    "- **Ground Truth Requirement:** \\( V \\)-measure assumes the availability of ground truth labels or known classes for evaluation. It is suited for scenarios where such information is available for comparison.\n",
    "\n",
    "In summary, the \\( V \\)-measure is a comprehensive metric in clustering evaluation that integrates both homogeneity and completeness to provide a balanced assessment of clustering quality, facilitating meaningful comparisons and interpretations of clustering results against ground truth or labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553eda7-4e21-439f-a73c-b663b6522b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a830c0-de29-4f87-9d30-604b0423b298",
   "metadata": {},
   "source": [
    "**Q3. How is the Silhouette Coefficient used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093ff2d-03b9-446a-8eda-78885a778305",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Silhouette Coefficient is a metric used to evaluate the quality of clustering results by measuring how well-separated clusters are and how similar data points are within the same cluster. It provides an indication of the cohesion (how close points in a cluster are to each other) and separation (how well-separated clusters are from each other) of the clustering result.\n",
    "\n",
    "### Calculation of Silhouette Coefficient:\n",
    "\n",
    "For each data point \\( i \\):\n",
    "\n",
    "1. **Calculate Cohesion \\( a(i) \\):**\n",
    "   - \\( a(i) \\) is the average distance between \\( i \\) and all other points in the same cluster \\( C_i \\):\n",
    "   \\[ a(i) = \\frac{1}{|C_i| - 1} \\sum_{j \\in C_i, j \\neq i} d(i, j) \\]\n",
    "   where \\( d(i, j) \\) is the distance between points \\( i \\) and \\( j \\).\n",
    "\n",
    "2. **Calculate Separation \\( b(i) \\):**\n",
    "   - \\( b(i) \\) is the average distance between \\( i \\) and all points in the nearest neighboring cluster \\( C_{\\text{nearest}} \\):\n",
    "   \\[ b(i) = \\min_{k \\neq i} \\left\\{ \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j) \\right\\} \\]\n",
    "\n",
    "3. **Compute Silhouette Coefficient \\( s(i) \\):**\n",
    "   \\[ s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}} \\]\n",
    "   - \\( s(i) \\) ranges from -1 to +1:\n",
    "     - \\( s(i) \\approx +1 \\): Data point \\( i \\) is well-clustered, with \\( a(i) \\) much smaller than \\( b(i) \\).\n",
    "     - \\( s(i) \\approx 0 \\): Data point \\( i \\) is on the boundary of two clusters.\n",
    "     - \\( s(i) \\approx -1 \\): Data point \\( i \\) may have been assigned to the wrong cluster.\n",
    "\n",
    "### Using Silhouette Coefficient for Evaluation:\n",
    "\n",
    "- **Overall Silhouette Coefficient \\( S \\):**\n",
    "  - Average of \\( s(i) \\) across all data points \\( i \\):\n",
    "  \\[ S = \\frac{1}{n} \\sum_{i=1}^{n} s(i) \\]\n",
    "  where \\( n \\) is the total number of data points.\n",
    "\n",
    "- **Interpretation:**\n",
    "  - Higher \\( S \\) indicates better clustering. A value close to +1 indicates dense, well-separated clusters.\n",
    "  - Negative values suggest that clusters may overlap or data points have been incorrectly clustered.\n",
    "\n",
    "### Range of Values:\n",
    "\n",
    "- \\( S \\) ranges from -1 to +1:\n",
    "  - \\( S = +1 \\): Best clustering quality, where clusters are well-separated and data points are correctly assigned to clusters.\n",
    "  - \\( S = 0 \\): Overlapping clusters or clusters with significant overlap.\n",
    "  - \\( S = -1 \\): Indicates clustering results where data points may have been incorrectly assigned to clusters.\n",
    "\n",
    "### Usage and Considerations:\n",
    "\n",
    "- **Comparison:** Use \\( S \\) to compare different clustering algorithms or parameter settings.\n",
    "- **Interpretation:** Helps in understanding the compactness and separation of clusters.\n",
    "- **Dependency:** \\( S \\) relies on distance metrics, so normalization of data and choice of distance metric can impact its interpretation.\n",
    "\n",
    "In summary, the Silhouette Coefficient provides a quantitative measure to assess the quality of clustering results, considering both cohesion within clusters and separation between clusters. It offers insights into the compactness and separation of clusters, aiding in the evaluation and comparison of clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9b61f-12fd-4485-a495-8b2e9ddc5571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d2c813-3138-487f-8f77-ab8ea795400f",
   "metadata": {},
   "source": [
    "**Q4. How is the Davies-Bouldin Index used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095e1e0-2f00-4615-87e5-123f6398bd22",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Davies-Bouldin Index (DBI) is another metric used to evaluate the quality of clustering results. It measures the average similarity between each cluster and its most similar cluster, based on both intra-cluster and inter-cluster distances.\n",
    "\n",
    "### Calculation of Davies-Bouldin Index:\n",
    "\n",
    "1. **Calculate Cluster Similarity \\( R_{ij} \\):**\n",
    "   - For each pair of clusters \\( i \\) and \\( j \\), calculate the cluster similarity \\( R_{ij} \\):\n",
    "   \\[ R_{ij} = \\frac{s_i + s_j}{d(c_i, c_j)} \\]\n",
    "   where:\n",
    "   - \\( s_i \\) and \\( s_j \\) are the average distances of points in clusters \\( i \\) and \\( j \\) to their respective cluster centers \\( c_i \\) and \\( c_j \\).\n",
    "   - \\( d(c_i, c_j) \\) is the distance between the cluster centers \\( c_i \\) and \\( c_j \\).\n",
    "\n",
    "2. **Compute Davies-Bouldin Index \\( DBI \\):**\n",
    "   - The Davies-Bouldin Index \\( DBI \\) is the average of the maximum similarity measure \\( R_{ij} \\) for each cluster \\( i \\):\n",
    "   \\[ DBI = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} R_{ij} \\]\n",
    "   where \\( k \\) is the number of clusters.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Lower \\( DBI \\) indicates better clustering:** A smaller \\( DBI \\) value suggests that clusters are well-separated (high inter-cluster similarity) and compact (low intra-cluster distance).\n",
    "- **Range of Values:** Theoretically, \\( DBI \\) ranges from 0 to \\( +\\infty \\):\n",
    "  - \\( DBI = 0 \\): Perfect clustering, where each cluster is perfectly separated from others.\n",
    "  - Higher \\( DBI \\) values indicate poorer clustering, with clusters that are either too spread out or overlapping.\n",
    "\n",
    "### Usage and Considerations:\n",
    "\n",
    "- **Comparison:** Use \\( DBI \\) to compare different clustering algorithms or parameter settings.\n",
    "- **Objective Function:** Minimizing \\( DBI \\) helps in finding clusters that are compact and well-separated.\n",
    "- **Dependency:** Like other metrics, \\( DBI \\) depends on the distance metric used and the normalization of data.\n",
    "\n",
    "### Practical Application:\n",
    "\n",
    "- **Evaluation:** \\( DBI \\) provides insights into the overall quality of clustering results by considering both intra-cluster cohesion and inter-cluster separation.\n",
    "- **Interpretation:** Helps in understanding the balance between compactness within clusters and separation between clusters.\n",
    "\n",
    "In summary, the Davies-Bouldin Index \\( DBI \\) is a useful metric for evaluating clustering results, focusing on the compactness and separation of clusters. It offers a quantitative measure to assess the quality of clustering outcomes, aiding in the selection and optimization of clustering algorithms for different datasets and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508e694-406e-4934-acda-741e2dc1d532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a89b3a3a-4519-4449-a6d8-23e3fbe9fa05",
   "metadata": {},
   "source": [
    "**Q5. Can a clustering result have a high homogeneity but low completeness? Explain with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6d411-d57e-443e-8d14-6a71e73b5e9a",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "\n",
    "Yes, it is possible for a clustering result to have high homogeneity but low completeness, although this scenario is less common compared to other combinations of homogeneity and completeness.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **Homogeneity:** Measures if all clusters contain only data points that are members of a single class.\n",
    "- **Completeness:** Measures if all data points that are members of a given class are assigned to the same cluster.\n",
    "\n",
    "### Example Scenario:\n",
    "\n",
    "Let's consider a hypothetical clustering result with three clusters and two classes:\n",
    "\n",
    "1. **Cluster 1 (High Homogeneity, Low Completeness):**\n",
    "   - Contains all data points from Class A.\n",
    "   - No data points from Class B are included in Cluster 1.\n",
    "   - Homogeneity is high because all points in Cluster 1 belong to Class A.\n",
    "   - Completeness is low because not all points of Class A are in Cluster 1; some are in other clusters.\n",
    "\n",
    "2. **Cluster 2 (High Homogeneity, Low Completeness):**\n",
    "   - Contains all data points from Class B.\n",
    "   - No data points from Class A are included in Cluster 2.\n",
    "   - Homogeneity is high because all points in Cluster 2 belong to Class B.\n",
    "   - Completeness is low because not all points of Class B are in Cluster 2; some are in other clusters.\n",
    "\n",
    "3. **Cluster 3 (High Completeness, Low Homogeneity):**\n",
    "   - Contains a mix of data points from both Class A and Class B.\n",
    "   - Homogeneity is low because Cluster 3 does not exclusively contain points from a single class.\n",
    "   - Completeness is high because all points from both Class A and Class B are grouped together in one cluster.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "In this example:\n",
    "- **High Homogeneity, Low Completeness Clusters (Cluster 1 and Cluster 2):** These clusters are very pure in terms of containing only one class but fail to include all members of that class, resulting in low completeness.\n",
    "  \n",
    "- **High Completeness, Low Homogeneity Cluster (Cluster 3):** This cluster includes all members of both classes, achieving high completeness. However, it lacks homogeneity because it mixes data points from different classes.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "While it's less typical to have high homogeneity and low completeness simultaneously, it can occur in clustering scenarios where clusters are highly pure in terms of class membership but fail to capture all members of a class in a single cluster. This illustrates the nuanced relationship between homogeneity and completeness in evaluating clustering results, emphasizing the need to consider both metrics together for a comprehensive assessment of clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f97fc7-8bc9-4988-af56-eee32fdce1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa8be0f-e5db-40d9-abbe-4b1e3d8287d4",
   "metadata": {},
   "source": [
    "**Q6. How can the V-measure be used to determine the optimal number of clusters in a clustering\n",
    "algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc4131-32ef-4a73-bf51-511f019777fd",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The V-measure can be utilized to determine the optimal number of clusters in a clustering algorithm by assessing clustering quality across different numbers of clusters and selecting the number that maximizes the V-measure score. Here’s how you can approach using V-measure for determining the optimal number of clusters:\n",
    "\n",
    "### Steps to Determine Optimal Number of Clusters:\n",
    "\n",
    "1. **Generate Clustering Results:**\n",
    "   - Apply the clustering algorithm with different numbers of clusters \\( k \\).\n",
    "   - Obtain clustering labels for each \\( k \\).\n",
    "\n",
    "2. **Compute V-measure for Each \\( k \\):**\n",
    "   - Calculate the homogeneity \\( H \\) and completeness \\( C \\) for each clustering result.\n",
    "   - Compute the V-measure \\( V \\) using the formula:\n",
    "     \\[ V = 2 \\cdot \\frac{H \\cdot C}{H + C} \\]\n",
    "   - Alternatively, you can use the scikit-learn library in Python, which provides a function `metrics.cluster.v_measure_score` to compute the V-measure directly.\n",
    "\n",
    "3. **Evaluate V-measure Scores:**\n",
    "   - Plot or analyze the V-measure scores against the number of clusters \\( k \\).\n",
    "   - Look for the point where the V-measure score stabilizes or reaches a peak.\n",
    "\n",
    "4. **Select Optimal Number of Clusters:**\n",
    "   - Choose the number of clusters \\( k \\) that corresponds to the highest V-measure score.\n",
    "   - This \\( k \\) value indicates the number of clusters that best balances both homogeneity and completeness, leading to the most meaningful clustering solution.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Balance Between Homogeneity and Completeness:** The optimal number of clusters determined by V-measure should reflect a balance where clusters are both internally cohesive (homogeneous) and well-separated (high completeness).\n",
    "\n",
    "- **Visualization and Interpretation:** Plotting V-measure scores can provide visual insights into how clustering quality varies with different numbers of clusters, aiding in decision-making.\n",
    "\n",
    "- **Scalability:** Depending on the dataset size and complexity, computing V-measure for a range of \\( k \\) values may require efficient clustering algorithms and computational resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb808b6-a9b9-4e3b-bdc4-b9b408d9c165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2, V-measure score: 1.0\n",
      "Number of clusters: 3, V-measure score: 0.8132898335036762\n",
      "Number of clusters: 4, V-measure score: 0.7162089270041652\n",
      "Number of clusters: 5, V-measure score: 0.6150762885445168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "# Example dataset X (replace with your actual dataset)\n",
    "X = np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])\n",
    "\n",
    "# Example true labels (ground truth), replace with your actual labels if available\n",
    "true_labels = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Example of determining optimal number of clusters using V-measure\n",
    "k_values = range(2, 6)\n",
    "v_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Fit KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Compute V-measure score\n",
    "    v_score = v_measure_score(true_labels, labels)\n",
    "    v_scores.append(v_score)\n",
    "\n",
    "# Print V-measure scores\n",
    "for k, score in zip(k_values, v_scores):\n",
    "    print(f\"Number of clusters: {k}, V-measure score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1ccb3-8cc1-466d-b167-934b400ebb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bde409a-f020-460b-9931-c71e751928c8",
   "metadata": {},
   "source": [
    "**Q7. What are some advantages and disadvantages of using the Silhouette Coefficient to evaluate a\n",
    "clustering result?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273323fc-56cb-45c8-aec9-b2fada7bd97d",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Silhouette Coefficient is a widely used metric for evaluating the quality of clustering results. Like any metric, it comes with its own set of advantages and disadvantages:\n",
    "\n",
    "### Advantages of Silhouette Coefficient:\n",
    "\n",
    "1. **Intuitive Interpretation:**\n",
    "   - The Silhouette Coefficient provides a clear and intuitive measure of how well-defined and separated clusters are. A higher value indicates that clusters are well-separated and points are more similar to their own cluster than to neighboring clusters.\n",
    "\n",
    "2. **Comprehensive Assessment:**\n",
    "   - It considers both the cohesion (how close points in a cluster are) and separation (how distinct clusters are from each other) aspects of clustering quality in a single metric.\n",
    "\n",
    "3. **Range and Interpretation:**\n",
    "   - The coefficient ranges from -1 to +1, where:\n",
    "     - \\( +1 \\) indicates well-clustered and distinct clusters,\n",
    "     - \\( 0 \\) indicates overlapping clusters,\n",
    "     - \\( -1 \\) indicates incorrect clustering where points may have been assigned to the wrong clusters.\n",
    "\n",
    "4. **Applicability to Various Algorithms:**\n",
    "   - It can be applied to a wide range of clustering algorithms, as long as distance or similarity measures are defined.\n",
    "\n",
    "### Disadvantages of Silhouette Coefficient:\n",
    "\n",
    "1. **Dependency on Distance Metric:**\n",
    "   - The Silhouette Coefficient heavily depends on the choice of distance metric. Different metrics can lead to different silhouette values, impacting comparability across studies or datasets.\n",
    "\n",
    "2. **Sensitive to Noise and Outliers:**\n",
    "   - Outliers and noise in the data can significantly impact silhouette scores, potentially reducing their effectiveness in noisy datasets.\n",
    "\n",
    "3. **Ambiguity in Interpretation:**\n",
    "   - In cases where clusters are of varying densities or shapes, the interpretation of silhouette scores can be ambiguous. High silhouette scores do not always guarantee the absence of overlapping or misclassified points.\n",
    "\n",
    "4. **Difficulty with High-dimensional Data:**\n",
    "   - In high-dimensional spaces, where the distance metrics can lose effectiveness (curse of dimensionality), the silhouette coefficient might become less reliable or informative.\n",
    "\n",
    "5. **Assumption of Euclidean Space:**\n",
    "   - The silhouette coefficient assumes that clusters are well-separated in a Euclidean space. In non-Euclidean spaces or when dealing with categorical data, modifications or alternative metrics might be required.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The Silhouette Coefficient remains a popular and valuable metric for assessing clustering quality due to its intuitive interpretation and ability to capture both cohesion and separation of clusters. However, its effectiveness can vary depending on data characteristics, such as dimensionality, noise levels, and clustering algorithm used. It is often recommended to complement silhouette analysis with other metrics (like Davies-Bouldin Index, V-measure, etc.) to gain a more comprehensive understanding of clustering performance across different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5e44c-9c7b-4ea2-869e-8317e998ec49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85bb9b2a-f065-4bea-bc11-52c1cfa03167",
   "metadata": {},
   "source": [
    "**Q8. What are some limitations of the Davies-Bouldin Index as a clustering evaluation metric? How can\n",
    "they be overcome?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c61169-0e16-42a9-b6c3-6308e7297d25",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Davies-Bouldin Index (DBI) is a useful metric for evaluating clustering results, but like any metric, it has its limitations:\n",
    "\n",
    "### Limitations of the Davies-Bouldin Index (DBI):\n",
    "\n",
    "1. **Dependence on Cluster Centers:**\n",
    "   - DBI heavily depends on the accurate estimation of cluster centers. If the cluster centers are not well-defined or incorrectly estimated, DBI may not accurately reflect the clustering quality.\n",
    "\n",
    "2. **Sensitivity to Number of Clusters:**\n",
    "   - DBI tends to favor solutions with a larger number of clusters because it is calculated as an average over pairwise cluster distances. This bias can lead to selecting too many clusters if not interpreted carefully.\n",
    "\n",
    "3. **Assumption of Convex Clusters:**\n",
    "   - DBI assumes that clusters are convex and well-separated. In reality, clusters may have complex shapes or overlap, which can affect the index's reliability.\n",
    "\n",
    "4. **Scalability with Dimensionality:**\n",
    "   - In high-dimensional spaces, where distance metrics can lose effectiveness (curse of dimensionality), DBI may become less reliable or informative.\n",
    "\n",
    "5. **Difficulty with Non-numeric Data:**\n",
    "   - DBI is typically designed for numeric data and may not be directly applicable to categorical or text data without appropriate transformations or adaptations.\n",
    "\n",
    "### Overcoming Limitations:\n",
    "\n",
    "While some limitations of DBI are inherent to its formulation, there are strategies to mitigate these issues or complement DBI with other metrics for a more comprehensive evaluation:\n",
    "\n",
    "1. **Improved Cluster Center Estimation:**\n",
    "   - Use robust methods for estimating cluster centers, such as iterative algorithms that minimize distance measures more accurately.\n",
    "\n",
    "2. **Normalization of Distance Measures:**\n",
    "   - Normalize distances based on cluster variances or densities to reduce bias towards larger clusters and improve sensitivity to cluster separability.\n",
    "\n",
    "3. **Alternative Distance Metrics:**\n",
    "   - Consider using different distance metrics or similarity measures that better capture the structure of the data, especially in high-dimensional or non-Euclidean spaces.\n",
    "\n",
    "4. **Ensemble or Consensus Clustering Approaches:**\n",
    "   - Combine DBI with other clustering evaluation metrics or ensemble clustering techniques to reduce bias and improve the reliability of clustering evaluations.\n",
    "\n",
    "5. **Domain-specific Adjustments:**\n",
    "   - Tailor DBI or develop domain-specific adaptations that address specific characteristics of data, such as non-numeric attributes or complex cluster shapes.\n",
    "\n",
    "6. **Visualization and Interpretation:**\n",
    "   - Use visualization techniques to complement DBI by visually inspecting cluster separations and overlaps, aiding in the interpretation of clustering results beyond numerical metrics alone.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The Davies-Bouldin Index remains a valuable tool for assessing clustering quality, particularly in scenarios where cluster convexity and separability are reasonable assumptions. While it has limitations, understanding these limitations and employing appropriate strategies can enhance its effectiveness and reliability in evaluating clustering algorithms across various datasets and applications. Integrating DBI with other metrics and techniques offers a more holistic approach to evaluating and validating clustering results comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6c782-3f3a-4be9-8489-20f1df8b01fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb618d4-c1c3-4fe0-a970-9d50e8cd914e",
   "metadata": {},
   "source": [
    "**Q9. What is the relationship between homogeneity, completeness, and the V-measure? Can they have\n",
    "different values for the same clustering result?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1dfe2-0442-429f-a468-8be57b58dffb",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Homogeneity, completeness, and the V-measure are metrics used to evaluate the quality of clustering results, focusing on different aspects of how well clusters align with ground truth labels (if available). Here's how they relate and whether they can have different values for the same clustering result:\n",
    "\n",
    "### Homogeneity:\n",
    "\n",
    "- **Definition:** Homogeneity measures if all clusters contain only data points that are members of a single class.\n",
    "- **Calculation:** It is computed as:\n",
    "  \\[ \\text{Homogeneity} = 1 - \\frac{H(C|K)}{H(C)} \\]\n",
    "  where \\( H(C|K) \\) is the conditional entropy of the class distribution given the cluster assignments \\( K \\), and \\( H(C) \\) is the entropy of the class distribution.\n",
    "\n",
    "### Completeness:\n",
    "\n",
    "- **Definition:** Completeness measures if all data points that are members of a given class are assigned to the same cluster.\n",
    "- **Calculation:** It is computed as:\n",
    "  \\[ \\text{Completeness} = 1 - \\frac{H(K|C)}{H(K)} \\]\n",
    "  where \\( H(K|C) \\) is the conditional entropy of the cluster assignment given the true class labels \\( C \\), and \\( H(K) \\) is the entropy of the cluster assignment.\n",
    "\n",
    "### V-measure:\n",
    "\n",
    "- **Definition:** V-measure is the harmonic mean of homogeneity and completeness, providing a balanced measure of both metrics.\n",
    "- **Calculation:** It is computed as:\n",
    "  \\[ V = 2 \\cdot \\frac{ \\text{Homogeneity} \\cdot \\text{Completeness} }{ \\text{Homogeneity} + \\text{Completeness} } \\]\n",
    "\n",
    "### Relationship and Differences:\n",
    "\n",
    "1. **Relationship:**\n",
    "   - Homogeneity and completeness are complementary measures that capture different aspects of clustering quality related to class purity and cluster assignment consistency.\n",
    "   - V-measure combines these two metrics into a single measure, balancing their contributions using harmonic mean.\n",
    "\n",
    "2. **Different Values for the Same Clustering Result:**\n",
    "   - Yes, homogeneity, completeness, and V-measure can have different values for the same clustering result because they evaluate different aspects:\n",
    "     - **Homogeneity** focuses on the purity of clusters in terms of class labels.\n",
    "     - **Completeness** focuses on how well clusters cover entire classes.\n",
    "     - **V-measure** combines these two aspects and can provide a different perspective on clustering quality than either homogeneity or completeness alone.\n",
    "   - Differences in values can arise due to the specific characteristics of the data, such as class distribution imbalance, cluster overlap, or the effectiveness of the clustering algorithm in capturing these aspects.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Homogeneity, completeness, and the V-measure are essential metrics for evaluating clustering results, each offering unique insights into the alignment between clusters and true class labels. While they are related, they measure distinct aspects of clustering quality and can provide complementary information when assessing and comparing different clustering algorithms or parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c25df4-9cb4-4282-bffa-b29ec904abae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42bdfa7-a412-434e-825b-c07cf7eb3107",
   "metadata": {},
   "source": [
    "**Q10. How can the Silhouette Coefficient be used to compare the quality of different clustering algorithms\n",
    "on the same dataset? What are some potential issues to watch out for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08301205-0395-4f6c-80b8-333cba3be14b",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Silhouette Coefficient is a metric used to assess the quality of individual clusters within a dataset. It can also be used to compare the overall quality of different clustering algorithms applied to the same dataset. Here’s how you can use the Silhouette Coefficient for comparing clustering algorithms and some potential issues to consider:\n",
    "\n",
    "### Using Silhouette Coefficient for Comparison:\n",
    "\n",
    "1. **Compute Silhouette Coefficient:**\n",
    "   - Apply each clustering algorithm to the dataset and compute the Silhouette Coefficient for each clustering result.\n",
    "\n",
    "2. **Aggregate Scores:**\n",
    "   - Calculate the average Silhouette Coefficient across all samples in the dataset for each clustering algorithm.\n",
    "\n",
    "3. **Compare Scores:**\n",
    "   - Compare the average Silhouette Coefficients obtained from different algorithms. A higher average score generally indicates better-defined clusters and better overall clustering quality.\n",
    "\n",
    "### Potential Issues to Watch Out For:\n",
    "\n",
    "1. **Dependency on Distance Metric:**\n",
    "   - The Silhouette Coefficient is sensitive to the choice of distance metric. Different metrics (e.g., Euclidean, Manhattan) can yield different Silhouette scores, affecting the comparability of clustering algorithms.\n",
    "\n",
    "2. **Interpretation Across Algorithms:**\n",
    "   - Different clustering algorithms may produce different cluster shapes and structures, impacting the interpretation of Silhouette scores. Algorithms that inherently produce spherical clusters may bias the Silhouette scores higher compared to those that handle non-convex clusters.\n",
    "\n",
    "3. **Scalability and Dataset Size:**\n",
    "   - Silhouette computation involves pairwise distances, which can be computationally expensive for large datasets. Ensure algorithms are efficient and scalable, especially when comparing on large datasets.\n",
    "\n",
    "4. **Cluster Density and Shape:**\n",
    "   - The Silhouette Coefficient assumes clusters of roughly equal size and shape. Algorithms that produce clusters of varying densities or non-spherical shapes may yield lower Silhouette scores, even if they are appropriate for the data.\n",
    "\n",
    "5. **Noise and Outliers:**\n",
    "   - Outliers or noise in the data can significantly affect Silhouette scores, potentially skewing comparisons between algorithms. Preprocessing steps such as outlier detection or robust clustering methods may be needed.\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Normalize Across Metrics:** Use consistent distance metrics and normalization techniques (if applicable) across algorithms to ensure fair comparison.\n",
    "  \n",
    "- **Visual Validation:** Supplement Silhouette scores with visual inspection of cluster distributions and shapes to understand how well clusters align with the data’s inherent structure.\n",
    "\n",
    "- **Ensemble Approaches:** Consider ensemble or consensus clustering methods to mitigate biases from individual clustering algorithms and enhance the robustness of comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211fa3a0-7d42-4663-9b4d-2d2feada17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: Average Silhouette score = 0.8469881221532085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs  # Example dataset generator\n",
    "\n",
    "# Generate example data (replace with your actual dataset loading)\n",
    "X, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n",
    "\n",
    "# Example of comparing clustering algorithms using Silhouette Coefficient\n",
    "algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "    # Add more algorithms as needed\n",
    "}\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    # Fit clustering algorithm\n",
    "    algorithm.fit(X)\n",
    "    \n",
    "    # Predict clusters\n",
    "    labels = algorithm.labels_\n",
    "    \n",
    "    # Compute Silhouette score\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    print(f\"{name}: Average Silhouette score = {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291493f8-6b74-471a-9f40-269ab2173ad0",
   "metadata": {},
   "source": [
    "**Q11. How does the Davies-Bouldin Index measure the separation and compactness of clusters? What are\n",
    "some assumptions it makes about the data and the clusters?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9c9b5-1ad6-4023-9421-efe0a2a69503",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "The Davies-Bouldin Index (DBI) is a clustering evaluation metric that quantifies the quality of a clustering solution based on the separation and compactness of clusters. Here’s how DBI measures these aspects and the assumptions it makes about the data and clusters:\n",
    "\n",
    "### Measurement of Separation and Compactness:\n",
    "\n",
    "1. **Separation:**\n",
    "   - DBI measures the average similarity between each cluster and the cluster that is most similar to it, considering their centroids and the distance between them. Lower values indicate better separation, where clusters are distinct and well-separated from each other.\n",
    "\n",
    "2. **Compactness:**\n",
    "   - DBI also considers the intra-cluster compactness, which measures how tightly grouped the data points within each cluster are around their centroid. Lower intra-cluster distances indicate more compact clusters.\n",
    "\n",
    "### Calculation of Davies-Bouldin Index:\n",
    "\n",
    "The DBI is calculated as the average over all clusters \\( i \\):\n",
    "\n",
    "\\[ \\text{DBI} = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{\\text{similarity}(i, j) + \\text{similarity}(j, i)}{\\text{distance}(c_i, c_j)} \\right) \\]\n",
    "\n",
    "- \\( k \\) is the number of clusters.\n",
    "- \\( c_i \\) and \\( c_j \\) are the centroids of clusters \\( i \\) and \\( j \\), respectively.\n",
    "- \\( \\text{similarity}(i, j) \\) measures the similarity between clusters \\( i \\) and \\( j \\), often computed using distance metrics.\n",
    "- \\( \\text{distance}(c_i, c_j) \\) measures the distance between centroids \\( c_i \\) and \\( c_j \\).\n",
    "\n",
    "### Assumptions of Davies-Bouldin Index:\n",
    "\n",
    "1. **Euclidean Distance:**\n",
    "   - DBI assumes the use of Euclidean distance or a similar metric that reflects the spatial separation and compactness of clusters. Non-Euclidean metrics may require adjustments or interpretations.\n",
    "\n",
    "2. **Convex Clusters:**\n",
    "   - The index assumes that clusters are convex and well-separated. Non-convex clusters or clusters with complex shapes may not be accurately evaluated by DBI.\n",
    "\n",
    "3. **Equal Cluster Sizes:**\n",
    "   - DBI assumes clusters of equal size and density, which may not always hold true in real-world datasets with varying densities and sizes.\n",
    "\n",
    "4. **Independent Features:**\n",
    "   - It assumes that features contributing to the clustering are independent and contribute equally to the distance calculation. Correlated or dependent features may bias DBI results.\n",
    "\n",
    "5. **Cluster Centroid Representativeness:**\n",
    "   - The index assumes that cluster centroids accurately represent the cluster members. Outliers or skewed distributions within clusters can affect centroid placement and thus DBI scores.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The Davies-Bouldin Index provides a quantitative measure of clustering quality by assessing both the separation and compactness of clusters. While useful for evaluating many clustering algorithms, its assumptions about data characteristics and cluster properties should be considered when interpreting results. Understanding these assumptions helps in applying DBI appropriately and in context to ensure meaningful evaluation of clustering solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5658289-8c06-459d-bead-fc64c9da72a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68ee02ab-cc32-409f-b612-dc183d15af37",
   "metadata": {},
   "source": [
    "**Q12. Can the Silhouette Coefficient be used to evaluate hierarchical clustering algorithms? If so, how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41b2f6-dad8-441b-aaa8-9031c058b8ec",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Yes, the Silhouette Coefficient can be used to evaluate hierarchical clustering algorithms, but the interpretation and application differ slightly compared to partitioning clustering algorithms like KMeans. Here’s how you can use the Silhouette Coefficient for hierarchical clustering:\n",
    "\n",
    "### Steps to Evaluate Hierarchical Clustering with Silhouette Coefficient:\n",
    "\n",
    "1. **Perform Hierarchical Clustering:**\n",
    "   - Use a hierarchical clustering algorithm (e.g., agglomerative clustering) to cluster your data. This algorithm creates a hierarchy of clusters that can be represented as a dendrogram.\n",
    "\n",
    "2. **Cut the Dendrogram:**\n",
    "   - Decide on the number of clusters or cut the dendrogram at a certain height or distance threshold to obtain a flat clustering assignment. This step determines the clusters you will evaluate.\n",
    "\n",
    "3. **Compute Silhouette Coefficient:**\n",
    "   - After obtaining the clustering assignments from hierarchical clustering, compute the Silhouette Coefficient for each sample. This metric measures how similar each sample is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "4. **Aggregate Scores:**\n",
    "   - Calculate the average Silhouette Coefficient across all samples in your dataset to obtain an overall measure of clustering quality.\n",
    "\n",
    "### Considerations for Hierarchical Clustering:\n",
    "\n",
    "- **Dendrogram Cutting:** The choice of where to cut the dendrogram affects the resulting clusters and thus the Silhouette Coefficient. Different cuts can yield different evaluation scores, so it's essential to choose a cutting method that aligns with your clustering goals or use multiple cuts for robust evaluation.\n",
    "\n",
    "- **Distance Metric:** Ensure consistency in the distance metric used for hierarchical clustering and Silhouette computation. Most hierarchical clustering algorithms support various distance metrics, including Euclidean, Manhattan, and others.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The Silhouette Coefficient is a versatile metric that can be applied to evaluate the quality of hierarchical clustering algorithms. By cutting the dendrogram appropriately and computing the Silhouette Coefficient on resulting clusters, you can assess how well the hierarchical clustering method partitions your data into meaningful clusters. Adjustments in the cutting strategy and consideration of distance metrics can enhance the accuracy and reliability of the evaluation process for hierarchical clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2bb01a-bc37-4ca5-a480-8fa7c1a42b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Silhouette score = 0.8469881221532085\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate example data (replace with your actual dataset loading)\n",
    "X, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n",
    "\n",
    "# Example of hierarchical clustering with Silhouette Coefficient\n",
    "# Using Agglomerative Clustering as an example\n",
    "cluster = AgglomerativeClustering(n_clusters=3)\n",
    "labels = cluster.fit_predict(X)\n",
    "\n",
    "# Compute Silhouette score\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(f\"Average Silhouette score = {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d4c3-e990-453a-a3eb-9f28f175a637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
